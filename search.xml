<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AQS相关]]></title>
    <url>%2F2019%2F03%2F12%2FAQS%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[AQS 简单介绍AbstractQueuedSynchronizer，抽象同步队列。是实现同步的基础组件，并发包中的锁都是基于 AQS 实现。 内部有一个 state 变量，用于表示一些状态信息，这个状态信息具体由实现类决定，比如一个 ReentrantLock 类这个 state 就表示获取锁的次数。 内部维持两个队列，Sync Queue 和 Condition Queue，Sync Queue 是一个双线 FIFO 链表，是锁的时候用到。而 Condition Queue 是条件队列，作为锁的等待条件时用到。 这个类使用到了模板方法设计模式：定义一个操作中算法的骨架，而将一些步骤的实现延迟到子类中。 AQSAQS 类简单介绍先看一下 AQS 的类图 AQS 是一个 FIFO 的双向队列，内部通过 tail 和 head 来记录队尾和队首，队列元素为 Node，状态信息为 state，通过内部类 ConditionObject 来结合锁实现线程同步 Node 节点：Note 的属性 thread 来存储进入 AQS 的线程（竞争锁失败进入等待队列）；Node 节点内部 SHARED 表示是获取共享资源被阻塞挂起后放入 AQS 队列的，EXCLUSIVE 表示获取独占资源被阻塞后挂起放入到 AQS 队列的；prev 记录当前节点的前驱节点，next 记录当前节点的后续节点。 waitStatus 表示当前线程等待状态，分别为 CANCELLED（线程被取消），SIGNAL（线程需要被唤醒），CONDITION（线程在条件队列里面等待），PROPAGATE（释放共享资源时候需要通知其他节点）， state 状态信息： AQS 中维持了一个单一的状态信息 state，可以通过 getState、setState 和 compareAndSetState 函数修改其值 ReentrantLock：当前线程获取锁的次数 ReentrantReadWriteLock：state 高16位表示读状态也就是获取读锁的线程数，低16位表示写状态也就是获取写锁的次数 Semaphore：当前可用信号个数 FutureTask：开始，运行，完成，取消 CountDownlatch 和 CyclicBarrie：计数器当前的值 ConditionObject 内部类： AQS 通过内部类 ConditionObject 来结合锁实现线程同步，ConditionObject 可以直接访问 AQS 内部变量（state 状态值和 Node 队列）。ConditionObject 是条件变量，每个条件变量对应一个条件队列（单向链表），线程调用条件变量的 await 方法后阻塞会放入到该队列，如类图，条件队列的头尾元素分别为 firstWaiter 和 lastWaiter。 AQS 实现同步原理AQS 通过操作 state 状态变量实现同步的操作。操作 state 分为共享模式和独占模式。 独占模式获取和释放锁方法为： 123void acquire(int arg) void acquireInterruptibly(int arg) boolean release(int arg) 共享模式获取和释放锁方法为： 123void acquireShared(int arg) void acquireSharedInterruptibly(int arg) boolean releaseShared(int arg) Interruptibly关键字的方法： 带 Interruptibly 关键字的方法会对中断进行相应，也就是在线程调用 acquireInterruptibly（或 acquireSharedInterruptibly） 方法获取资源时或获取资源失败被挂起的时候，其它线程中断了该线程，那么该线程会抛出 InterruptedException 异常而返回。 独占模式获取锁与共享模式获取锁区别： 独占模式获取锁是与具体线程绑定的，比如独占锁 ReentrantLock，如果线程获取到锁，会通过 CAS 操作将 state 从0变为1并且将当前锁的持有者设为该线程。当该线程再次获取锁时发现锁的持有者为自己，就会将 state +1，当另外的线程尝试获取锁，发现当前锁持有者不是自己，会被放入到 AQS 队列并挂起。 共享模式获取锁是与具体线程不相关的，多个线程请求请求资源时候是通过 CAS 方式竞争获取的，也就是说 CAS 操作只要成功就 OK。比如 Semaphore 信号量，当一个线程通过 acquire() 方法获取一个信号量时候，会首先看当前信号量个数是否满足需要，不满足则把当前线程放入阻塞队列，如果满足则通过自旋 CAS 获取信号量。 独占模式 acquire 源码分析12345public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 这是一个模板方法,获取锁 tryAcquire(arg) 的具体实现定义在子类中。 获取到锁 tryAcquire 就直接返回，否则调用 addWaiter 将当前节点添加到等待队列末尾。 1234567891011121314151617181920212223242526272829303132333435private Node addWaiter(Node mode) { // 将当前线程封装为 Node，设为独占模式 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // 如果 tail 不为空，将 node 插入末尾 if (pred != null) { node.prev = pred; // 可能多个线程同时插入，用 CAS 操作 if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 如果 tail 节点为空，或调用 CAS 操作将 当前节点设为 tail 节点失败 enq(node); return node;}private Node enq(final Node node) { for (;;) { Node t = tail; // 可能多个线程同时插入，重新判断是否为空 if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } }} addWaiter 和 enq 方法是为了将当前线程 node 插入到队列末尾。插入成功后不会立即挂起当前线程，因为在 addWaiter 过程中前面的线程可能已经执行完。此时会调用自选操作 acquireQueued 让该线程尝试重新获取锁，如果获取锁成功就退出，否则继续。 123456789101112131415161718192021222324 final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); // 如果其前驱节点为头结点，尝试获取锁，将该节点设为头结点，然后返回 if (p == head && tryAcquire(arg)) { // Called only by acquire methods setHead(node); p.next = null; // help GC failed = false; return interrupted; } // 如果获取锁失败，则判断是否需要挂起 if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed) cancelAcquire(node); }} 先尝试获取锁，失败再判断是否需要挂起，这个判断是通过它的前驱节点 waitStatus 确定的。 123456789101112131415161718192021222324252627private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; if (ws > 0) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node.prev = pred = pred.prev; } while (pred.waitStatus > 0); pred.next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false;} 如果前驱节点的 waitStatus 为： SINGAL，其前驱节点将要被唤醒，该节点可以安全的挂起，直接返回 true > 0，其前驱节点被取消，轮训将所有被取消的前驱节点都剔除，然后返回 false < 0，其前驱节点为 0 或 PROPAGATE，将前驱节点置为 SINGAL 表示自己将处于阻塞状态（下次判断时，会走 ws == Node.SIGNAL 的分支），然后返回 false。 返回 false，表示会重新执行 acquireQueued 方法，然后再重新检查前驱是不是头结点重新try一下什么的，也是之前描述的流程。 获取独占锁过程总结： AQS的模板方法acquire通过调用子类自定义实现的tryAcquire获取同步状态失败后->将线程构造成Node节点(addWaiter)->将Node节点添加到同步队列对尾(addWaiter)->节点以自旋的方法获取同步状态(acquirQueued)。在节点自旋获取同步状态时，只有其前驱节点是头节点的时候才会尝试获取同步状态，如果该节点的前驱不是头节点或者该节点的前驱节点是头节点单获取同步状态失败，则判断当前线程需要阻塞，如果需要阻塞则需要被唤醒过后才返回。 独占模式 release 源码分析AQS 的 release 释放同步状态和 acquire 获取同步状态一样，都是模板方法，tryRealease 具体操作都由子类实现，父类 AQS 只是提供一个算法骨架。 123456789public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false;} 如果释放成功，会解锁头节点的后续节点。 1234567891011121314151617181920212223242526272829private void unparkSuccessor(Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; if (ws < 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node.next; // 如果后续节点为空或是作废节点 if (s == null || s.waitStatus > 0) { s = null; // 从末尾开始找合适的节点 for (Node t = tail; t != null && t != node; t = t.prev) if (t.waitStatus MAX_COUNT) throw new Error("Maximum lock count exceeded"); // Reentrant acquire setState(c + acquires); return true; } // 读写锁都未被获取 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;} writerShouldBlock 方法对于非公平锁，是直接返回false，这样就会走 CAS 操作，与别的所有线程一起竞争，也就是后来的线程与先来的线程一起“插队”竞争。writerShouldBlock 方法对于公平锁，会调用 hasQueuedPredecessors 会判断是否有前驱节点，如果有则直接放回放弃进竞争，毕竟别人先来的要公平。 写锁的释放： 释放写锁会调用 release 方法，release 是 AQS 的模板方法，tryRelease 由子类实现，我们来看tryRelease。 1234567891011121314protected final boolean tryRelease(int releases) { // 判断是否是写锁拥有者 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 获取释放锁后的 可重入 值 int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; // 如果可重如值为0，就释放锁 if (free) setExclusiveOwnerThread(null); // 写锁只有一个线程，不需要 CAS 操作 setState(nextc); return free;}tryAcquireShared 读锁的获取和释放读锁通过 ReadLock 来实现，读锁是共享锁。 读锁的获取： 调用 AQS 的 acquireShared 方法，内部调用 ReentrantReadWriteLock 中的 Sync 重写的 tryAcquireShared 方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950protected final int tryAcquireShared(int unused) { /* * Walkthrough: * 1. If write lock held by another thread, fail. * 2. Otherwise, this thread is eligible for * lock wrt state, so ask if it should block * because of queue policy. If not, try * to grant by CASing state and updating count. * Note that step does not check for reentrant * acquires, which is postponed to full version * to avoid having to check hold count in * the more typical non-reentrant case. * 3. If step 2 fails either because thread * apparently not eligible or CAS fails or count * saturated, chain to version with full retry loop. */ Thread current = Thread.currentThread(); int c = getState(); // 如果写锁被获取，且不是当前线程 if (exclusiveCount(c) != 0 && getExclusiveOwnerThread() != current) return -1; // 获取了读锁的线程数，别搞成是可重入数量 int r = sharedCount(c); // 尝试获取读锁，多个线程只有一个成功，不成功的会进入下面的 fullTryAcquireShared 方法 if (!readerShouldBlock() && r < MAX_COUNT && compareAndSetState(c, c + SHARED_UNIT)) { // 第一个获取读锁 if (r == 0) { firstReader = current; firstReadeHoldCount = 1; // 是第一个获取读锁的线程 } else if (firstReader == current) { firstReaderHoldCount++; } else { HoldCounter rh = cachedHoldCounter; // 将获取到读锁的当前线程和可重入数记录到 cachedHoldCounter 中 if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); // cachedHoldCounter 为当前线程，将其保存到 readHolds 中 else if (rh.count == 0) readHolds.set(rh); rh.count++; } return 1; } // 自旋 CAS 获取读锁 return fullTryAcquireShared(current);} readerShouldBlock 方法会判断是否需要阻塞，在公平锁和非公平锁有不同的实现。 在非公平锁下，如果同步等待队列中有获取写锁的线程在排队，则获取读锁的该线程会阻塞，否则直接尝试获取读锁。 1234567891011121314static final class NonfairSync extends Sync { final boolean readerShouldBlock() { return apparentlyFirstQueuedIsExclusive(); }}final boolean apparentlyFirstQueuedIsExclusive() { Node h, s; return (h = head) != null && // AQS 第一个 node 是虚拟节点 (s = h.next) != null && !s.isShared() && s.thread != null;} 在公平锁下，如果同步队列中有其他线程在排队，则获取读锁的该线程会阻塞，这里和写锁是一样的，先来后到~ 12345static final class FairSync extends Sync { final boolean readerShouldBlock() { return hasQueuedPredecessors(); }} 读锁的释放： 直接看 tryReleaseShared 方法 123456789101112131415161718192021222324252627282930313233protected final boolean tryReleaseShared(int unused) { Thread current = Thread.currentThread(); // 先 firstReader 再 cachedHoldCounter 最后 readHolds if (firstReader == current) { // assert firstReaderHoldCount > 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; } else { HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; // 为0 记得在 ThreadLocal 中 remove if (count]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>锁</tag>
        <tag>源码分析</tag>
        <tag>AQS</tag>
        <tag>Lock</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java中的锁]]></title>
    <url>%2F2019%2F03%2F05%2Fjava%E4%B8%AD%E7%9A%84%E9%94%81%2F</url>
    <content type="text"><![CDATA[公平锁和非公平锁公平锁好处是等待锁的线程不会饿死，但整体效率较低。非公平锁好处是整体效率高一些，但是有些线程可能需要等待很久才能获取到锁。因为非公平锁是可以抢占的。 两者内部都维持一个 AQS 等待队列（FIFO），放置等待等待获取锁的线程。如果释放锁的时候，没有新的线程来获取锁，这时候就会从 AQS 队列头取出线程让其获取到锁。这时候公平锁和非公平锁是一样的。如果释放锁的时候，刚好有新的线程来获取锁，非公平锁就会将锁分配给这个新的线程，这就是非公平锁的抢占式。因为非公平锁中新来的线程有一定几率不会被挂起，减少了整体线程挂起的几率，所以非公平锁性能高于公平锁。 公平锁可以使用new ReentrantLock(true)实现。 自旋锁java 的线程是映射到操作系统的，线程的阻塞和唤醒需要操作系统来完成，这就需要从用户态转换到核心态，增加了状态切换的耗时。许多应用的共享数据的锁定状态自会持续很短的时间，为了这点时间去挂起和恢复现场不值得。我们可以让请求锁的线程自旋等待，不放弃CPU时间，直到获取到锁。 自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK6中已经变为默认开启，并且引入了自适应的自旋锁。自适应意味着自旋的时间不在固定了，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 自旋锁是通过 CAS 实现的，即不断重试 CAS 直到成功为止。自旋锁存在的问题如下： 占用过多 CPU 时间 死锁问题，在递归调用中，同一个线程获取到自旋锁，由再次申请获取 ABA 问题，java 中自旋锁一般是利用 CAS 操作实现 jdk 中 atomic 包下都是采用自旋锁原理 锁消除锁消除是指 JVM 在 JIT 编译时，通过扫描上下文，去除不可能存在共享资源竞争的锁。通过锁消除，可以减少无畏的请求锁时间。 锁消除的主要判断依据是来源于逃逸分析的数据支持，如果判断在一段代码中，堆上的所有数据都不会逃逸出去从而能被其他线程访问到，那就可以把他们当做栈上数据对待，认为他们是线程私有的，同步加锁自然就无需进行。 比如下面这个代码 public String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } JIT 回去除 append 方法的加锁 锁粗化一般情况下，都是推荐加锁范围越小越好。但是如果连续几行代码都是对同一个对象加锁解锁，甚至加锁操作出现循环体中。虚拟机遇到这样的情况，会将加锁范围扩大到整个操作序列的外部 比如下面的代码 StringBuffer sb = new StringBuffer(); public String concatString(String s1, String s2, String s3) { StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString(); } 因为 StringBuffer 定义在方法体外面，存在锁竞争，每个 append 都会加解锁，jvm 会将加锁范围扩大到 整个 append 操作序列的外部 可重入锁可重入锁也叫递归锁，同一线程可以多次获取同一个锁。可重入锁避免了死锁，synchronizez 和 ReentrantLock 都是可重入锁 类锁和对象锁 public class SynchronizedTest { public static synchronized void method1() {} public void method2() { synchronized(LockStrategy.class) {} } public synchronized void method3() {} } 其中 method1 和 method2 都是类锁，method3 是对象锁 偏向锁、轻量级锁和重量级锁在 jdk6 之前， synchronizez 一直是一个重量级锁，在 jdk6 中对 synchronized 做了很多优化，引入了偏向锁和轻量级锁。 对象头锁存在于 java 对象头中，如果对象是数组类型，则虚拟机用 3 个 Word（字宽）存储对象头，如果对象是非数组类型，则用 2 字宽存储对象头。在 32 位虚拟机中，一字宽等于四字节，即 32bit。 长度 内容 说明 32/64 bit Mark Word HashCode，分代年龄和锁标记位 32/64 bit Class Metadata Address 存储对象的类型指针 32/64 bit Array length 数组的长度（如果当前对象是数组） Class Metadata Address 用于存储对象的类型指针，该指针指向它的类元数据，JVM通过这个指针确定对象是哪个类的实例 Mark WordJava 对象头里的 Mark Word 里默认存储对象的 HashCode，分代年龄和锁标记位。32位 JVM 的 Mark Word 存储结果如下 其中 epoch 为偏向时间戳。 偏向锁偏向锁是为了消除无竞争情况下的同步原语，进一步提升程序性能。 偏向锁会偏向于第一个获取到他的线程，只要没有别的线程获取该锁，那么这第一个线程将永远不需要同步。 public synchronized void method() { // do ... } 如上述代码，线程A 第一个获取到这个锁，那么线程A后续执行 method，就不需要再进行获取锁的操作（只要中途没有别的线程来获取锁）。当有线程B来执行 method 时，偏向锁宣告结束，进入轻量级锁。 实现原理：当线程请求到锁对象后，将锁对象的状态标志位改为01，即偏向模式。然后使用CAS操作将线程的ID记录在锁对象的Mark Word中。以后该线程可以直接进入同步块，连CAS操作都不需要。但是，一旦有第二条线程需要竞争锁，那么偏向模式立即结束，进入轻量级锁的状态。 优点：偏向锁可以调高有同步但是没有竞争的程序性能，但是如果锁对象同时被多个线程竞争，那么偏向锁是多余的 偏向锁可以通过 JVM 参数来关闭：-XX:-UseBiasedLocking=false 轻量级锁使用 CAS 操作代替互斥同步，在线程请求锁时 实现原理：在线程请求锁时，判断锁对象的 Mark Word 是否是无锁状态（锁标志位为01，是否偏向为0），然后在线程的栈帧中创建一块 Lock Record 空间，并将锁对象的 Mark Word 复制到 Lock Record中，然后通过 CAS 操作将锁对象的 Mark Word 替换为指向 Lock Record 的指针，并将 Lock Record 的 owner 指针指向 锁对象的 Mark Word。如下图： 获取锁失败的处理：如果成功则表示获取锁成功，如果失败，首先会检查锁对象的 Mark Word 是否指向当前线程的栈帧。如果是表示获取锁成功，如果还不是表示获取锁失败。此时轻量级锁膨胀为重量级锁，当前线程会尝试用自旋获取锁，后面的线程会阻塞等待。 前提：轻量级锁比重量级锁性能高的前提是，在轻量级锁被占用期间，不会发生锁的竞争。一旦发生锁竞争，会膨胀为重量级锁，除了使用互斥量外还额外增加了 CAS 操作。 获取锁失败了，为什么要重复检查 Mark Word因为可能是可重入锁 三者区别 重量级锁是一种悲观锁，而轻量级锁和偏向锁是乐观锁 轻量级锁是在无锁竞争情况下，使用 CAS 操作来代替互斥量使用。而偏向锁是在无锁竞争情况下，完全取消同步（只有第一次获取锁的时候会使用 CAS 操作） 轻量级锁适用场景是线程交替进入同步块，如果同一时间多个线程竞争同一把锁就会膨胀为重量级锁 一个线程重复访问同步块，轻量级锁每次都要进行 CAS 操作。而偏向锁是为了避免在无锁竞争情况下，不必要的 CAS 操作。只有第一次获取锁的时候会使用 CAS 操作 synchronized 通过监视器锁来实现同步（monitorenter 和 monitorexit），而监视器锁有依赖于底层的互斥锁，进入互斥锁需要用户态与和心态的切换，所以synchronized是重量级锁 乐观锁和悲观锁悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。 乐观锁：假定不会发生并发冲突，只在提交操作时检测是否违反数据完整性。（使用版本号或者时间戳来配合实现） 读写锁读写锁是数据库常见的锁，又叫 共享-排它锁，S锁和X锁 共享锁：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排它锁。获准共享锁的事务只能读数据，不能修改数据。 排它锁：如果事务T对数据A加上排它锁后，则其他事务不能再对A加任何类型的锁。获得排它锁的事务即能读数据又能修改数据。 读锁不能直接升级为写锁，需要重新获取写锁。 Java当中的读写锁通过ReentrantReadWriteLock实现 互斥锁同一时刻最多只有一个线程持有锁，在JDK中synchronized和JUC的Lock就是互斥锁。 无锁有些方法不涉及到共享数据，就不会出现线程安全问题，一定线程安全的有： 无状态编程 ThreadLocal等线程封闭方案 volatile（volatile只能保证可见性和防止重排序，并不能保证线程安全） CAS 协程，单线程内维持多个任务的调度 分段锁ConcurrentHashMap 闭锁闭锁是一种同步工具类，可以延迟线程的进度直到其到达终止状态。闭锁的作用相当于一扇门：在闭锁到达结束状态之前，这扇门一直是关闭的，并且没有任何线程能通过，当到达结束状态时，这扇门会打开允许所有的线程通过。当闭锁到达结束状态后，将不会再改变状态，因此这扇门将永远保持打开状态。闭锁可以用来确保某些活动指导其他活动都完成后才继续执行。CountDownLatch就是一种灵活的闭锁实现。 死锁多个线程因资源竞争而相互等待的现象。出现死锁必须满足4个条件： 互斥条件：一个资源一次只能被一个进程使用 请求与保持条件：一个进程因请求资源而阻塞等待，对已获取的资源保持不放 不剥夺条件：进程已获取的资源，不会被剥夺 循环等待条件：若干进程形成循环等待资源的关系。 活锁LiveLock是一种形式活跃性问题，该问题尽管不会阻塞线程，但也不能继续执行，因为线程将不断重复执行相同的操作，而且总会失败。活锁通常发送在处理事务消息的应用程序中：如果不能成功地处理某个消息，那么消息处理机制将回滚整个事务，并将它重新放到队列的开头：如果不能成功地处理某个消息，那么消息处理机制将回滚整个事务，并将它重新放到队列的开头。如果消息处理器在处理某种特定类型的消息时存在错误并导致它失败，那么每当这个消息从队列中取出并传递到存在错误的处理器时，都会发生事务回滚。由于这条消息又被放回到队列开头，因此处理器将被反复调用，并返回相同的结果。 参考https://www.infoq.cn/article/java-se-16-synchronized https://www.zhihu.com/question/55075763 https://hiddenpps.blog.csdn.net/article/details/51204385]]></content>
      <categories>
        <category>java进阶</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo deploy 指定 git 账户]]></title>
    <url>%2F2019%2F03%2F04%2Fhexodeploy%E6%8C%87%E5%AE%9Agit%E8%B4%A6%E6%88%B7%2F</url>
    <content type="text"><![CDATA[问题hexo deploy默认使用全局的git user.name user.email，通过设置自定义 git 用户 操作在 hexo 全局 _config.yml 中添加配置 12345678deploy: type: git repo: branch: [branch] message: [message] name: [git user] email: [git email] extend_dirs: [extend directory] 然后需要删除 .deploy_git 目录，重新 hexo deploy 生成即可 参考https://github.com/hexojs/hexo/issues/2125]]></content>
      <categories>
        <category>计算机操作</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java内存模型]]></title>
    <url>%2F2019%2F02%2F28%2Fjava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[基础现代计算机物理内存模型 访问局部性（英语：Locality of reference） 访问局部性分为两种基本形式，一种是时间局部性，另一种是空间局部性。时间局部性指的是，程序在运行时，最近刚刚被引用过的一个内存位置容易再次被引用，比如在调取一个函数的时候，前不久才调取过的本地参数容易再度被调取使用。空间局部性指的是，最近引用过的内存位置以及其周边的内存位置容易再次被使用。空间局部性比较常见于循环中，比如在一个数列中，如果第3个元素在上一个循环中使用，则本次循环中极有可能会使用第4个元素。 指令重排序指令重排序是为了提高程序性能做得优化，比如多次写操作，每次都要会写内存，可以在线程的 working memory 操作完成后，一起回写内存。 指令重排序包括： 编译器优化重排序 指令级并行重排序 内存系统的重排序 as-if-serial无论如何重排序,程序执行的结果都应该与代码顺序执行的结果一致(Java编译器,运行时和处理器都会保证java在单线程下遵循as-if-serial语义). 线程的 working memory是 cache 和寄存器的抽象，解释源于《Concurrent Programming in Java: Design Principles and Patterns, Second Edition》，而不单单是内存的某个部分 Java 内存模型（Java Memory Model）Java内存模型(Java Memory Model)描述了Java程序中各种变量(线程共享变量)的访问规则,以及在JVM中将变量存储到内存和从内存中读取出变量这样的底层细节. happens-before 原则规定了 java 指令操作的偏序关系。是 JMM 制定的一些偏序关系，用于保证内存的可见性。 8大 happens-before 原则： 单线程happen-before原则：在同一个线程中，书写在前面的操作happen-before后面的操作。 锁的happen-before原则：同一个锁的unlock操作happen-before此锁的lock操作。 volatile的happen-before原则：对一个volatile变量的写操作happen-before对此变量的任意操作(当然也包括写操作了)。 happen-before的传递性原则：如果A操作 happen-before B操作，B操作happen-before C操作，那么A操作happen-before C操作。 线程启动的happen-before原则：同一个线程的start方法happen-before此线程的其它方法。 线程中断的happen-before原则：对线程interrupt方法的调用happen-before被中断线程的检测到中断发送的代码。 线程终结的happen-before原则：线程中的所有操作都happen-before线程的终止检测。 对象创建的happen-before原则：一个对象的初始化完成先于他的finalize方法调用。 内存可见性共享变量实现可见性原理线程1对共享变量的修改对线程2可见，需要2个步骤： 将工作内存1中修改的共享变量刷新到主内存 将主内存最新的共享变量更新到工作内存2 synchronizedJMM 关于synchronized 的两条规定： 线程解锁前，刷新共享变量到主存 线程加锁前，获取主存中共享变量最新值到工作内存 volatile有内存栅栏（或内存屏障）和防止指令重排序 JMM 中，在 volatile 变量写操作后加入 store 栅栏（(强制将变量值刷新到主内存中去)），在读操作前加入 load 栅栏（强制从主内存中读取变量的值） 参考https://www.jianshu.com/p/1508eedba54dhttps://www.jianshu.com/p/47f999a7c280http://ifeve.com/easy-happens-before/]]></content>
      <categories>
        <category>java进阶</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>jmm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发编程之美——相关总结]]></title>
    <url>%2F2019%2F02%2F28%2FJava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8B%E7%BE%8E%E2%80%94%E2%80%94%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[伪共享在 cpu 和 内存之间有高速缓存区（cache）,cache 一般集成在 cpu 内部，也叫做 cpu Cache。如下图是一个二级缓存示意图。 cache 内部是按照行来存储的，每行称为一个 cache 行，大小为2的n次幂，一个 cache 行 可能会存有多个变量数据 当多个线程同时修改一个 cache 行里面的多个变量时候，由于同时只能有一个线程操作缓存行，所以相比每个变量放到一个缓存行性能会有所下降，这就是伪共享。 当单个线程顺序访问同一个 cache 行的多个变量，利用程序运行局部性原理会加快程序运行。当多个程序同时访问同一个 cache 行的多个变量，会发生竞争，速度会慢]]></content>
      <categories>
        <category>java进阶</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F02%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>java</tag>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
